{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96dc8464-fa68-4c14-a8d1-d92269cb6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "from io import StringIO\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f94ad3-be4b-43a4-8bfc-6bc3b7927ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to measure execution of a given block of code:\n",
    "# with catchtime():\n",
    "#   # code here\n",
    "class catchtime:\n",
    "    def __enter__(self):\n",
    "        self.start = perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.time = perf_counter() - self.start\n",
    "        self.readout = f'Time: {self.time:.3f} seconds'\n",
    "        print(self.readout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5587f3-db0f-4695-ae4f-939238f2db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5ba025-9b3f-42ce-81e6-5147d74f102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"jp_en_sentences.tsv\", 'r', encoding='utf-8-sig') as f:\n",
    "    raw = f.read()\n",
    "    raw = raw.replace(\"”\", \"\\\"\")\n",
    "    raw = raw.replace(\"“\", \"\\\"\")\n",
    "    rd = csv.DictReader(StringIO(raw), delimiter=\"\\t\")\n",
    "    for row in rd:\n",
    "        sentences.append((row['jp'], row['en']))\n",
    "jp = [jp for jp, _ in sentences]\n",
    "en = [en for _, en in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f6b844-fb3b-4c52-b49e-6c2e57978404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentence pairs:  264482\n"
     ]
    }
   ],
   "source": [
    "print(\"num sentence pairs: \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417ce9da-bef3-49a1-852d-d83c6d069b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 520\n",
    "BOS = \"BOS\"\n",
    "EOS = \"EOS\"\n",
    "jp_chars = sorted(list(set(\"\".join([jp for jp, en in sentences]))))\n",
    "jp_vocab_size = len(jp_chars)\n",
    "jp_stoi = { ch:i for i,ch in enumerate(jp_chars) }\n",
    "jp_stoi[EOS] = len(jp_stoi)\n",
    "jp_stoi[BOS] = len(jp_stoi)\n",
    "jp_itos = { i:ch for i,ch in enumerate(jp_chars) }\n",
    "jp_itos[len(jp_stoi)-2] = EOS\n",
    "jp_itos[len(jp_stoi)-1] = BOS\n",
    "\n",
    "JP_VOCAB_SIZE = len(jp_stoi)\n",
    "\n",
    "\n",
    "def encode_jp(sent):\n",
    "    enc = [jp_stoi[BOS]] + [jp_stoi[c] for i, c in enumerate(sent)]\n",
    "    enc += [jp_stoi[EOS]] * (block_size - len(enc))\n",
    "    return enc\n",
    "def decode_jp(encoded):\n",
    "    return \"\".join([jp_itos[c] for c in encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac57054-fc8f-4808-bcf9-e0e214ee2e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOSきみにちょっとしたものをもってきたよ。EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_jp(encode_jp(sentences[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b353aed4-33ee-4457-95f8-7981821dbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedListIter:\n",
    "    def __init__(self, head):\n",
    "        self.head = head\n",
    "    def __next__(self):\n",
    "        if not self.head:\n",
    "            raise StopIteration\n",
    "        curr = self.head\n",
    "        self.head = self.head.next\n",
    "        return curr\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self, contents):\n",
    "        self.contents = contents\n",
    "        self.next = None\n",
    "        self.cached_full = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create(chunk):\n",
    "        curr = None\n",
    "        head = None\n",
    "        for c in chunk:\n",
    "            new = LinkedList(c)\n",
    "            if curr:\n",
    "                curr.next = new\n",
    "            else:\n",
    "                head = new\n",
    "            curr = new\n",
    "        head.cached_full = chunk\n",
    "        return head\n",
    "    def merge(self):\n",
    "        assert self.next\n",
    "        self.contents += self.next.contents\n",
    "        self.next = self.next.next\n",
    "\n",
    "    def __iter__(self):\n",
    "        return LinkedListIter(self)\n",
    "    def __repr__(self):\n",
    "        return f\"LL{{{self.contents}, {self.next}}}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ddc08b-2ad0-4417-a518-df7a0f6107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(chunks, weights):\n",
    "    counter = collections.Counter()\n",
    "    for chunk_i in range(len(chunks)):\n",
    "        chunk = chunks[chunk_i]\n",
    "        if not chunk.next:\n",
    "            continue\n",
    "        for c1, c2 in zip(chunk, chunk.next):\n",
    "            counter[(c1.contents, c2.contents)] += weights[chunk.cached_full]\n",
    "    return counter            \n",
    "        \n",
    "def merge(chunks, bigram, cnts, weights):\n",
    "    cnts[bigram] = 0\n",
    "    for chunk in chunks:\n",
    "        head = chunk\n",
    "        prev = None\n",
    "        while head:\n",
    "            if not head.next:\n",
    "                break\n",
    "            c1, c2 = head.contents, head.next.contents\n",
    "            if (c1, c2) == bigram:\n",
    "                head.merge()\n",
    "                if prev:\n",
    "                    cnts[(prev.contents, head.contents)] += weights[chunk.cached_full]\n",
    "                if head.next:\n",
    "                    cnts[(head.contents, head.next.contents)] += weights[chunk.cached_full]\n",
    "            prev = head\n",
    "            head = head.next\n",
    "\n",
    "def seed_merges(sentences):\n",
    "    out = {}\n",
    "    for sent in sentences:\n",
    "        for c in sent:\n",
    "            if c not in out:\n",
    "                out[c] = len(out)\n",
    "    return out\n",
    "\n",
    "def count_occurences(splits):\n",
    "    counter = collections.Counter()\n",
    "    for sent in splits:\n",
    "        for chunk in sent:\n",
    "            counter[chunk] += 1\n",
    "    return counter\n",
    "\n",
    "PRE_TOKENIZATION_REGEX = re.compile(r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\w+|\\s|\\S\")\n",
    "\n",
    "def bpe(merges, sentences, vocab_size):\n",
    "    splits = [re.findall(PRE_TOKENIZATION_REGEX, s) for s in sentences]\n",
    "    weights = count_occurences(splits)\n",
    "    chunks = list(weights.keys())\n",
    "    for i in range(len(chunks)):\n",
    "        chunks[i] = LinkedList.create(chunks[i])\n",
    "    cnts = count_bigrams(chunks, weights)\n",
    "    for merge_id in range(vocab_size):\n",
    "        top = cnts.most_common(1)\n",
    "        if len(top) == 0:\n",
    "            return merges\n",
    "        top = top[0][0]\n",
    "        merges[\"\".join(top)] = len(merges)\n",
    "        merge(chunks, top, cnts, weights)\n",
    "    merges[EOS] = len(merges)\n",
    "    merges[BOS] = len(merges)\n",
    "    return merges\n",
    "\n",
    "def encode_en(sent, merges):\n",
    "    chunks = re.findall(PRE_TOKENIZATION_REGEX, sent)\n",
    "    for i in range(len(chunks)):\n",
    "        chunks[i] = LinkedList.create(chunks[i])\n",
    "    out = []\n",
    "    for chunk in chunks:\n",
    "        head = chunk\n",
    "        while head:\n",
    "            if not head.next:\n",
    "                break\n",
    "            c1, c2 = head.contents, head.next.contents\n",
    "            if (c1 + c2) in merges:\n",
    "                head.merge()\n",
    "            head = head.next\n",
    "    out = []\n",
    "    for chunk in chunks:\n",
    "        for c in chunk:\n",
    "            out.append(merges[c.contents])\n",
    "    out = [merges[BOS]] + out\n",
    "    out += [merges[EOS]] * (block_size - len(out))\n",
    "    return out\n",
    "\n",
    "def decode_en(enc, merges_reversed):\n",
    "    return \"\".join(merges_reversed[id] for id in enc)\n",
    "\n",
    "def print_encoded_chunks(encoded, merges_reversed):\n",
    "    \",\".join(rev[id] for id in enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc2ddef-8987-4342-8cc5-23c4728a8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre BPE size: 147\n",
      "Time: 22.962 seconds\n",
      "Dict size: 1149\n"
     ]
    }
   ],
   "source": [
    "CNT = len(sentences)\n",
    "with catchtime():\n",
    "    merges = seed_merges([en for _, en in sentences])\n",
    "    print(f\"Pre BPE size: {len(merges)}\")\n",
    "    merges = bpe(merges, [en for _, en in sentences[:CNT]], 1000)\n",
    "    merges_reversed = {i: seq for seq, i in merges.items()}\n",
    "EN_VOCAB_SIZE = len(merges)\n",
    "print(f\"Dict size: {len(merges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b962a48-801a-4d87-a656-64eb918d8076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.000 seconds\n"
     ]
    }
   ],
   "source": [
    "VALIDATE_ROUNDTRIP = False\n",
    "with catchtime():\n",
    "    if VALIDATE_ROUNDTRIP:\n",
    "        for jp, en in sentences:\n",
    "            assert (roundtripped := decode_en(encode_en(en, merges), merges_reversed)).startswith(\"BOS\" + en), f\"{roundtripped=} {en=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6f3fae-5e47-4f92-b3b9-d4e97f53191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "TEST_SIZE = 100\n",
    "TRAIN_SIZE = len(sentences) - TEST_SIZE\n",
    "train_data = sentences[:TRAIN_SIZE]\n",
    "val_data = sentences[TRAIN_SIZE:]\n",
    "\n",
    "def tensorify(sentence_pairs, merges):\n",
    "    jp = [encode_jp(jp) for jp, _ in sentence_pairs]\n",
    "    en = [encode_en(en, merges) for _, en in sentence_pairs]\n",
    "    jp_t, en_t = torch.tensor(jp), torch.tensor(en)\n",
    "    Y_train = torch.zeros_like(en_t)\n",
    "    Y_train[:, :-1] = en_t[:, 1:]\n",
    "    Y_train[:, -1] = merges[EOS]\n",
    "    return jp_t.to(device), en_t.to(device), Y_train.to(device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62eea708-c42d-48b2-9943-47057a98fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "def get_train_data():\n",
    "    pairs = []\n",
    "    for _ in range(batch_size):\n",
    "        ix = random.randint(0, TRAIN_SIZE-1)\n",
    "        pairs.append(train_data[ix])\n",
    "    return pairs\n",
    "\n",
    "def get_batch(split, merges):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = get_train_data() if split == 'train' else val_data\n",
    "    return tensorify(data, merges)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_BROKEN(merges):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "Xjp_test, Xen_test, Y_test = get_batch('test', merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3ffd382-7d4f-418a-873e-20d3686fee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890109 M parameters\n"
     ]
    }
   ],
   "source": [
    "n_emb = 64\n",
    "n_heads = 4\n",
    "n_layer = 4\n",
    "learning_rate = 1e-3\n",
    "dropout = .1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, masked=True):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.masked = masked\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        weight = query @ key.transpose(-2, -1) * C ** -.5\n",
    "\n",
    "        if self.masked and not self.training:\n",
    "            weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        return weight @ value\n",
    "\n",
    "class CrossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x_dec, x_enc):\n",
    "        _, _, C = x_enc.shape\n",
    "        query = self.query(x_dec)\n",
    "        key = self.key(x_enc)\n",
    "        value = self.value(x_enc)\n",
    "        weight = query @ key.transpose(-2, -1) * C ** -.5\n",
    "\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        return weight @ value\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, masked=True):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, masked) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class MultiCrossHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CrossHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x_dec, x_enc):\n",
    "        out = torch.cat([h(x_dec, x_enc) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FFBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        assert n_emb % num_heads == 0, f\"num_heads should divide n_emb evenly, found {n_emb}%{num_heads} = {n_emb % num_heads}\"\n",
    "        head_size = n_emb // num_heads\n",
    "        self.att = MultiHead(num_heads, head_size)\n",
    "        self.cross_attn = MultiCrossHead(num_heads, head_size)\n",
    "        self.ff = FFBlock()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "        self.ln3 = nn.LayerNorm(n_emb)\n",
    "        \n",
    "    def forward(self, x, x_enc):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.cross_attn(self.ln2(x), x_enc)\n",
    "        x = x + self.ff(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        assert n_emb % num_heads == 0, f\"num_heads should divide n_emb evenly, found {n_emb}%{num_heads} = {n_emb % num_heads}\"\n",
    "        head_size = n_emb // num_heads\n",
    "        self.att = MultiHead(num_heads, head_size, masked=False)\n",
    "        self.ff = FFBlock()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, num_stacks, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(num_heads=num_heads) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, x_dec, x_enc):\n",
    "        for layer in self.layers:\n",
    "            x_dec = layer(x_dec, x_enc)\n",
    "        return x_dec\n",
    "            \n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.enc_token_embedding_table = nn.Embedding(JP_VOCAB_SIZE, n_emb)\n",
    "        self.enc_position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.dec_token_embedding_table = nn.Embedding(EN_VOCAB_SIZE, n_emb)\n",
    "        self.dec_position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.decoder_blocks = DecoderStack(n_layer, n_heads)\n",
    "        self.encoder_blocks = nn.Sequential(*[EncoderBlock(num_heads=n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_emb) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_emb, EN_VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, enc_idx, dec_idx, targets=None):\n",
    "        B, T_enc = enc_idx.shape\n",
    "        _, T_dec = dec_idx.shape\n",
    "\n",
    "        # both (B,T_enc) tensor of integers\n",
    "        enc_tok_emb = self.enc_token_embedding_table(enc_idx) # (B,T_enc,C)\n",
    "        enc_pos_emb = self.enc_position_embedding_table(torch.arange(T_enc, device=device)) # (T_enc,C)\n",
    "\n",
    "        x_enc = enc_tok_emb + enc_pos_emb\n",
    "\n",
    "        y_enc = self.encoder_blocks(x_enc)\n",
    "\n",
    "        dec_tok_emb = self.dec_token_embedding_table(dec_idx) # (B,T_dec,C)\n",
    "        dec_pos_emb = self.dec_position_embedding_table(torch.arange(T_dec, device=device)) # (T_dec,C)\n",
    "\n",
    "        x_dec = dec_tok_emb + dec_pos_emb # (B,T_dec,C)\n",
    "        # HERE: x_enc -> y_enc\n",
    "        x = self.decoder_blocks.forward(x_dec, y_enc) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = Model()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "losses=[]\n",
    "test_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bee0bbf0-2b5b-4cf8-a8f7-be75c1a18875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_PATH = \"model.pt\"\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH).state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35f29953-b7f6-4f6e-91c9-47bc8cb95265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/100000 [00:00<6:57:36,  3.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 elapsed 0s: train: 0.1512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20002/100000 [54:10<3:37:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20000 elapsed 3250s: train: 0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40002/100000 [1:48:09<2:41:45,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40000 elapsed 6489s: train: 0.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60002/100000 [2:42:21<1:47:54,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 elapsed 9741s: train: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80002/100000 [3:36:19<53:56,  6.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80000 elapsed 12980s: train: 0.0641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [4:30:16<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 99999 elapsed 16217s: train: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 20000\n",
    "max_iters = 100000\n",
    "start = time.time()\n",
    "for iter in tqdm.tqdm(range(max_iters)):\n",
    "    Xjp_train, Xen_train, Y_train = get_batch('train', merges)\n",
    "    logits, loss = m(Xjp_train, Xen_train, Y_train)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        #_, test_loss = m(Xjp_test, Xen_test, Y_test)\n",
    "        #test_losses.append(test_loss.item())\n",
    "        #print(f\"step {iter} elapsed {time.time() - start:.0f}s: train: {loss.item():.4f} test: {test_loss.item():.4f}\")\n",
    "        test_losses.append(loss.item())\n",
    "        print(f\"step {iter} elapsed {time.time() - start:.0f}s: train: {loss.item():.4f}\")\n",
    "    else:\n",
    "        test_losses.append(test_losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4699972-193b-4987-918a-d3d8f92ec0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c303f6c3-b436-4c69-8b33-0e9650329730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=\"私は一日に100ユーロ稼ぎます。\"\n",
      "translated='BOSI enjoy 100 usual days in ten days.EOS'\n",
      "expected='I make €100 a day.'\n",
      "\n",
      "input=\"私は一日に100ユーロ稼ぎます。\"\n",
      "translated='BOSI am to earn to earnest under a day.EOS'\n",
      "expected='I make 100 euros per day.'\n",
      "\n",
      "input=\"すぐに諦めて昼寝をするかも知れない。\"\n",
      "translated='BOSWe want to keep to sleep on your day after lunch.EOS'\n",
      "expected='I may give up soon and just nap instead.'\n",
      "\n",
      "input=\"すぐに諦めて昼寝をするかも知れない。\"\n",
      "translated='BOSWe may know whether to a right nappo appreciates seat.EOS'\n",
      "expected='I may give up soon and just take a nap.'\n",
      "\n",
      "input=\"それはあなたが一人になりたくないからです。\"\n",
      "translated=\"BOSIt isn't so well get there an everything.EOS\"\n",
      "expected=\"It's because you don't want to be alone.\"\n",
      "\n",
      "input=\"それはあなたが一人になりたくないからです。\"\n",
      "translated='BOSThat will be a good person out of you.EOS'\n",
      "expected=\"That's because you don't want to be alone.\"\n",
      "\n",
      "input=\"そんなことは起きないでしょう。\"\n",
      "translated=\"BOSI won't join so so much.EOS\"\n",
      "expected=\"That won't happen.\"\n",
      "\n",
      "input=\"そんなことは起きないでしょう。\"\n",
      "translated=\"BOSDon't get it up so that.EOS\"\n",
      "expected=\"That sort of thing won't happen.\"\n",
      "\n",
      "input=\"彼は時々変です。\"\n",
      "translated='BOSHe is a praise.EOS'\n",
      "expected='Sometimes he can be a strange guy.'\n",
      "\n",
      "input=\"彼は時々変です。\"\n",
      "translated='BOSHe is my wife on change.EOS'\n",
      "expected=\"He's strange sometimes.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response(jp, merges, generator):\n",
    "    assert jp.shape[0] == 1\n",
    "    model.eval()\n",
    "    idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "    idx += merges[BOS]\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(block_size):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(jp, idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1, generator=generator) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        if idx_next[0][0].item() == merges[EOS]:\n",
    "            break\n",
    "    model.train()\n",
    "    return idx\n",
    "\n",
    "def translate(jp, expected=None, generator=None):\n",
    "    encoded_jp = encode_jp(jp)\n",
    "    translated = decode_en(generate_response(torch.tensor([encoded_jp], device=device), merges, generator).tolist()[0], merges_reversed)\n",
    "    s = f\"input=\\\"{jp}\\\"\\n{translated=}\\n\"\n",
    "    if expected:\n",
    "        s += f\"{expected=}\"\n",
    "    return s\n",
    "start = 30\n",
    "n = 10\n",
    "g = torch.Generator(device=device)\n",
    "g.manual_seed(0)\n",
    "for i in range(n):\n",
    "    print(translate(sentences[start + i][0], sentences[start + i][1], g))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e2a8912-1040-4210-b627-0140dc8168b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input=\"こんにちは\"\\ntranslated=\\'BOSAll.EOS\\'\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"こんにちは\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
